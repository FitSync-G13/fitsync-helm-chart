name: opensearch-observability-cluster

on:
  workflow_dispatch:
    inputs:
      cluster_name:
        description: K3s cluster name (used for artifact naming)
        required: true
        default: obs-opensearch
      k3s_version:
        description: K3s version
        required: false
        default: v1.29.4+k3s1
      master_instances:
        description: Comma-separated EC2 instance IDs for masters (1 minimum)
        required: true
      worker_instances:
        description: Comma-separated EC2 instance IDs for workers
        required: true
      k3s_api_dns:
        description: Public DNS name for the K3s API load balancer
        required: true
      aws_region:
        description: AWS region
        required: false
        default: us-east-1
      opensearch_version:
        description: OpenSearch version for the operator-managed cluster
        required: false
        default: 2.13.0
      domain:
        description: Base domain for the cluster (e.g., obs.fitsync.online)
        required: false
        default: obs.fitsync.online
      dashboards_host:
        description: FQDN for OpenSearch Dashboards ingress
        required: false
        default: osd.obs.fitsync.online
      api_host:
        description: FQDN for OpenSearch API ingress
        required: false
        default: opensearch-api.obs.fitsync.online
      acme_email:
        description: Email for ACME registration
        required: false
        default: sre@fitsync.online
      acme_server:
        description: ACME directory URL (use staging for tests)
        required: false
        default: https://acme-v02.api.letsencrypt.org/directory
      storage_class:
        description: StorageClass name for EBS volumes
        required: false
        default: gp3-ebs
      master_volume_size:
        description: Volume size for master nodes
        required: false
        default: 50Gi
      data_volume_size:
        description: Volume size for data nodes
        required: false
        default: 200Gi

env:
  KUBECONFIG: ${{ runner.temp }}/kubeconfig
  NAMESPACE: observability

jobs:
  k3s-install:
    name: Install K3s (masters + workers)
    uses: FitSync-G13/fitsync-cd-templates/.github/workflows/k3s-install-modular.yml@main
    with:
      cluster_name: ${{ inputs.cluster_name }}
      k3s_version: ${{ inputs.k3s_version }}
      master_instances: ${{ inputs.master_instances }}
      worker_instances: ${{ inputs.worker_instances }}
      k3s_api_dns: ${{ inputs.k3s_api_dns }}
      aws_region: ${{ inputs.aws_region }}
    secrets:
      AWS_ROLE_ARN: ${{ secrets.AWS_ROLE_ARN }}

  bootstrap-observability:
    name: Bootstrap observability stack
    runs-on: ubuntu-latest
    needs: k3s-install
    permissions:
      id-token: write
      contents: read
    env:
      STORAGE_CLASS: ${{ inputs.storage_class }}
      MASTER_VOLUME: ${{ inputs.master_volume_size }}
      DATA_VOLUME: ${{ inputs.data_volume_size }}
      DASHBOARDS_HOST: ${{ inputs.dashboards_host }}
      API_HOST: ${{ inputs.api_host }}
      OPENSEARCH_VERSION: ${{ inputs.opensearch_version }}
      BASE_DOMAIN: ${{ inputs.domain }}
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: ${{ secrets.AWS_ROLE_ARN }}
          aws-region: ${{ inputs.aws_region }}

      - name: Download kubeconfig from K3s install job
        uses: actions/download-artifact@v4
        with:
          name: ${{ inputs.cluster_name }}-kubeconfig
          path: ${{ runner.temp }}

      - name: Prepare kubeconfig
        run: |
          mkdir -p "$(dirname "$KUBECONFIG")"
          mv "${{ runner.temp }}/kubeconfig" "$KUBECONFIG"
          chmod 600 "$KUBECONFIG"

      - name: Add Helm repos
        run: |
          helm repo add jetstack https://charts.jetstack.io
          helm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx
          helm repo add aws-ebs-csi-driver https://kubernetes-sigs.github.io/aws-ebs-csi-driver
          helm repo update

      - name: Create namespaces and StorageClass
        env:
          AWS_REGION: ${{ inputs.aws_region }}
        run: |
          kubectl create namespace "$NAMESPACE" --dry-run=client -o yaml | kubectl apply -f -
          kubectl create namespace ingress-nginx --dry-run=client -o yaml | kubectl apply -f -

          cat <<EOF | kubectl apply -f -
          apiVersion: storage.k8s.io/v1
          kind: StorageClass
          metadata:
            name: ${STORAGE_CLASS}
          provisioner: ebs.csi.aws.com
          parameters:
            type: gp3
            encrypted: "true"
          reclaimPolicy: Delete
          volumeBindingMode: WaitForFirstConsumer
          allowVolumeExpansion: true
          EOF

      - name: Install cert-manager
        run: |
          kubectl apply -f https://github.com/cert-manager/cert-manager/releases/download/v1.15.1/cert-manager.crds.yaml
          helm upgrade --install cert-manager jetstack/cert-manager \
            --namespace cert-manager --create-namespace \
            --set installCRDs=false

      - name: Provision Route53 credentials secret for ACME DNS-01
        run: |
          kubectl -n cert-manager delete secret route53-credentials --ignore-not-found
          kubectl -n cert-manager create secret generic route53-credentials \
            --from-literal=aws_access_key_id='${{ secrets.ROUTE53_ACCESS_KEY_ID }}' \
            --from-literal=aws_secret_access_key='${{ secrets.ROUTE53_SECRET_ACCESS_KEY }}'

      - name: Create ClusterIssuer for wildcard certs
        env:
          ACME_EMAIL: ${{ inputs.acme_email }}
          ACME_SERVER: ${{ inputs.acme_server }}
          AWS_REGION: ${{ inputs.aws_region }}
        run: |
          cat <<EOF | kubectl apply -f -
          apiVersion: cert-manager.io/v1
          kind: ClusterIssuer
          metadata:
            name: letsencrypt-dns
          spec:
            acme:
              email: ${ACME_EMAIL}
              server: ${ACME_SERVER}
              privateKeySecretRef:
                name: letsencrypt-dns
              solvers:
                - dns01:
                    route53:
                      region: ${AWS_REGION}
                      hostedZoneID: ""
                      accessKeyIDSecretRef:
                        name: route53-credentials
                        key: aws_access_key_id
                      secretAccessKeySecretRef:
                        name: route53-credentials
                        key: aws_secret_access_key
          EOF

      - name: Request wildcard certificate
        run: |
          cat <<EOF | kubectl apply -f -
          apiVersion: cert-manager.io/v1
          kind: Certificate
          metadata:
            name: wildcard-${BASE_DOMAIN//./-}
            namespace: ${NAMESPACE}
          spec:
            secretName: wildcard-tls
            issuerRef:
              name: letsencrypt-dns
              kind: ClusterIssuer
            dnsNames:
              - "*.${BASE_DOMAIN}"
              - "${BASE_DOMAIN}"
          EOF

      - name: Install ingress-nginx (for OpenSearch Dashboards/API)
        run: |
          helm upgrade --install ingress-nginx ingress-nginx/ingress-nginx \
            --namespace ingress-nginx \
            --set controller.service.type=LoadBalancer \
            --set controller.ingressClassResource.name=nginx \
            --set controller.ingressClassResource.controllerValue=k8s.io/ingress-nginx \
            --set controller.metrics.enabled=true

      - name: Install AWS EBS CSI driver
        run: |
          helm upgrade --install aws-ebs-csi-driver aws-ebs-csi-driver/aws-ebs-csi-driver \
            --namespace kube-system \
            --set controller.serviceAccount.create=true \
            --set controller.serviceAccount.name=ebs-csi-controller-sa \
            --set controller.serviceAccount.annotations."eks\.amazonaws\.com/role-arn"='${{ secrets.AWS_EBS_CSI_ROLE_ARN }}' \
            --set imagePullPolicy=IfNotPresent

      - name: Install OpenSearch Operator
        run: |
          kubectl apply -f https://raw.githubusercontent.com/opensearch-project/opensearch-operator/main/deploy/crds/opensearch.opster.io_opensearchclusters.yaml
          kubectl apply -f https://raw.githubusercontent.com/opensearch-project/opensearch-operator/main/deploy/operator.yaml

      - name: Create OpenSearch cluster (1 master + 1 data, dashboards enabled)
        env:
          ADMIN_PASSWORD: ${{ secrets.OPENSEARCH_ADMIN_PASSWORD }}
        run: |
          kubectl -n ${NAMESPACE} delete secret opensearch-admin-credentials --ignore-not-found
          kubectl -n ${NAMESPACE} create secret generic opensearch-admin-credentials \
            --from-literal=admin_password="${ADMIN_PASSWORD}"

          cat <<EOF | kubectl apply -f -
          apiVersion: opensearch.opster.io/v1
          kind: OpenSearchCluster
          metadata:
            name: obs-os
            namespace: ${NAMESPACE}
          spec:
            general:
              version: ${OPENSEARCH_VERSION}
              httpPort: 9200
              serviceName: opensearch-api
              setVMMaxMapCount: true
            security:
              tls:
                transport:
                  generate: true
                http:
                  generate: false
                  secret:
                    name: wildcard-tls
              config:
                securityadmin:
                  adminSecret: opensearch-admin-credentials
                  adminPasswordKey: admin_password
            dashboards:
              enable: true
              replicas: 1
              tls:
                secret:
                  name: wildcard-tls
              ingress:
                host: ${DASHBOARDS_HOST}
                tls:
                  enable: true
                  secret:
                    name: wildcard-tls
            nodePools:
              - component: masters
                replicas: 1
                roles:
                  - cluster_manager
                resources:
                  requests:
                    cpu: "1"
                    memory: 2Gi
                persistence:
                  pvc:
                    storageClass: ${STORAGE_CLASS}
                    size: ${MASTER_VOLUME}
              - component: data
                replicas: 1
                roles:
                  - data
                  - ingest
                resources:
                  requests:
                    cpu: "2"
                    memory: 4Gi
                persistence:
                  pvc:
                    storageClass: ${STORAGE_CLASS}
                    size: ${DATA_VOLUME}
          EOF

      - name: Expose OpenSearch API via ingress
        run: |
          cat <<EOF | kubectl apply -f -
          apiVersion: networking.k8s.io/v1
          kind: Ingress
          metadata:
            name: opensearch-api
            namespace: ${NAMESPACE}
            annotations:
              kubernetes.io/ingress.class: nginx
              nginx.ingress.kubernetes.io/backend-protocol: HTTPS
              cert-manager.io/cluster-issuer: letsencrypt-dns
          spec:
            tls:
              - hosts:
                  - ${API_HOST}
                secretName: wildcard-tls
            rules:
              - host: ${API_HOST}
                http:
                  paths:
                    - path: /
                      pathType: Prefix
                      backend:
                        service:
                          name: opensearch-api
                          port:
                            number: 9200
          EOF


